{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Daipe? \u00b6 Daipe is an Enterprise AI Platform which helps you to prepare data, build ML models and productionalize them in the enterprise environment fast. Learn how to create a first Daipe project \u2192","title":"Daipe AI Platform Documentation"},{"location":"advanced/","text":"Advanced Daipe features \u00b6 Check our best-practices for setting up Daipe on MS Azure Manage & monitor quality of your data using SettleDQ","title":"Overview"},{"location":"azure-logger-bundle/","text":"Logging into Azure setup \u00b6 The Azure logger bundle package allows you to save logs from your Daipe project into Azure Application Insights which persists your project's logs in a queryable and visualizable way. That allows you to monitor your application's progress and health with ease. Installation \u00b6 poetry add azure-logger-bundle Usage \u00b6 Get the instrumentation key In your project's src/[ROOT_MODULE]/_config/config.yaml parameters : azureloggerbundle : enabled : True app_insights : instrumentation_key : xxxx-xxxx-xxxx-xxxx or use an environment variable instrumentation_key : \"%env(MY_SECRET_ENV)%\" Logs example \u00b6","title":"Logging into Azure setup"},{"location":"azure-setup-overview/","text":"Process Overview \u00b6 The schema above describes the full process of Daipe AI Platform setup: Setting up Azure Environment Spinning up Data Lake Resources Creating Data Pipelines project","title":"Setup overview"},{"location":"azure-setup/","text":"Setting up Azure Environment \u00b6 Go to setup steps \u2193 Prerequisites \u00b6 Subscription with Owner permissions Sufficient permissions to AAD (Azure Active Directory) - to create app registrations / service principals Azure DevOps project with Project Administrator permissions Best practises \u00b6 Subscription per environment We consider as a best practice to have separate subscriptions for dev/test/prod environments. Naming of service principals Our recommendation for naming is that the service principal name should reflect purpose and it's permission scope, eg. devops-service-connection-to-{subscription-name} devops-service-connection-to-{subscription}-{resource-group-name} Naming of service connections in DevOps We like to keep name for DevOps service connection same as the name of service principal we are authenticating through. Security Architecture \u00b6 This diagram helps to visualize what is going to be done. It only shows it for dev environment. Notice This tutorial describes how to setup dev environment. Setup for test and prod is identical. 1. Create service principal in Azure Active Directory \u00b6 Go to Azure portal Click on Azure Active Directory Click on App registrations Click on New registration Fill the name and click Register In this case the service principal's purpose is to authorize from DevOps to Azure Cloud and the service principal permission scope is to subscription dslabsdev hence the name devops-service-connection-to-dslabsdev . Copy Application ID and store it for later use Notice Assigning permission scope for service principal will be done in next steps so don't worry if it doesn't make sense yet. 2. Grant service principal permissions to Azure Active Directory API \u00b6 Click on API Permissions Click on Add a permission Click on Microsoft Graph Click on Application permissions Click on Application Select Application.ReadWrite.OwnedBy Click Add permissions You need to repeat this process do grant same permission to legacy API. Reason for this that some Microsoft tools still using this legacy API. These changes must be approved by some AAD Administrator. Security considerations These permissions only allow to read and manage applications created by service principal. So there is no risk that this service principal can be miused to read or modify any other AAD information. 3. Generate Application secret \u00b6 Click on Certificates & secrets Click on New client secret Add secret description Select expiration - Never Click Add Copy the secret value and store it for later use 4. Grant newly created service principal Owner permissions to your subscription \u00b6 In Azure portal click on Subscriptions You might need to uncheck global subscription filter Click on your dev subscription Click on Access control (IAM) Click on Add Click on Add role assignment Select Owner role Find your devops service principal Click on that service principal Click Save Why Owner role? The pipeline for deploying Resources uses post deploy script to set up secure integration between Databricks, Key Vault, Storage and this can be done only when Service principal has Owner role set. Security considerations Giving service principal permissions on subscription level might be risky. The Owner/Contributor role allow any creation or deletion of resources in that subscription so if you have another projects/resources in that subscription the service principal might be misused to delete them. Assign permissions at subscription level only if the subscription is empty. 5. Create service connection in Azure DevOps \u00b6 Go to any Azure DevOps project Click on settings Click on Service connections Click on New service connection Select Azure Resource Manager Click on Next Select Service principal (manual) Click on Next Fill in approptiate information Service Principal ID = Application ID that was stored in previous steps Service Principal key = Secret that was stored in previous steps You can find Subscription ID in Azure portal under Subscriptions You can find Tenant ID in Azure portal in Active Directory Overview Sevice connection name should be same as service principal name Uncheck Grant access permission to all pipelines Click on Verify and save 6. Create environments \u00b6 Create dev/test/prod environments Set approvals on prod environment 7. Make sure that your subscriptions have appropriate resource providers registered \u00b6 Explicit list , you can use it to filter the Resources: Microsoft.Notebooks Microsoft.ManagedIdentity Microsoft.ContainerRegistry Microsoft.ContainerInstance Microsoft.MachineLearningServices microsoft.insights Microsoft.PolicyInsights Microsoft.Storage Microsoft.Network Microsoft.DataFactory Microsoft.OperationalInsights Microsoft.Databricks Microsoft.KeyVault Microsoft.Security Microsoft.Compute Microsoft.ChangeAnalysis Microsoft.Advisor Microsoft.Commerce Microsoft.ClassicStorage Microsoft.MachineLearning Microsoft.ADHybridHealthService Microsoft.ResourceGraph Microsoft.Resources Microsoft.SerialConsole","title":"Environment setup"},{"location":"chaining-notebook-functions/","text":"Chaining decorated functions \u00b6 Calls of the decorated functions can be chained by passing function names as decorator arguments: @transformation ( read_table ( \"bronze.tbl_customers\" )) def tbl_customers ( df : DataFrame ): return df @transformation ( tbl_customers ) def active_customers_only ( df : DataFrame ): return df . filter ( f . col ( \"active\" ) == 1 ) @transformation ( active_customers_only , display = True ) def save ( df : DataFrame ): return df More compact way: @transformation ( read_table ( \"bronze.tbl_customers\" ), display = True ) def tbl_active_customers ( df : DataFrame ): return df . filter ( f . col ( \"active\" ) == 1 ) Once you run the active_customers_only function's cell, it gets is automatically called with the dataframe loaded by the customers_table function. Similarly, once you run the save_output function's cell, it gets automatically called with the filtered dataframe returned from the active_customers_only function.","title":"Chaining notebook functions"},{"location":"clone-demo-project/","text":"Get the Daipe demo project \u00b6 Cloning project from repository \u00b6 Cloning a Daipe demo project by running the following command: git clone https://github.com/daipe-ai/daipe-demo-databricks.git Prerequisites The following software needs to be installed first: Miniconda package manager Git for Windows or standard Git in Linux ( apt-get install git ) We recommend using the following IDEs: PyCharm Community or Pro with the EnvFile plugin installed Visual Studio Code with the PYTHONPATH setter extension installed Tu run commands, use Git Bash on Windows or standard Terminal on Linux/Mac Configure the local project In src/daipedemo/_config/config_dev.yaml fill Databricks URL. Create .env file from .env.dist and fill Databricks Personal Access Token. Initialize local environment Now run ./env-init.sh which will initialize your local environment. Activate the environment Now activate the Conda environment for your new project $ conda activate $PWD /.venv or use a shortcut $ ca","title":"Cloning demo project"},{"location":"coding-daipe-way/","text":"Coding the \"Daipe way\" \u00b6 Daipe greatly simplify datalake(house) management : Tools to simplify & automate table creation, updates and migrations. Explicit table schema enforcing for Hive tables, CSVs, ... Decorators to write well-maintainable and self-documented function-based notebooks Rich configuration options to customize naming standards, paths, and basically anything to match your needs Why function based notebooks? Compared to bare notebooks, the function-based approach brings the following advantages : create and publish auto-generated documentation and lineage of notebooks and pipelines write much cleaner notebooks with properly named code blocks (unit)test specific notebook functions with ease use YAML to configure your notebooks for given environment (dev/test/prod/...) utilize pre-configured objects to automate repetitive tasks","title":"Intro"},{"location":"create-daipe-project/","text":"Creating project from skeleton \u00b6 Create a new Daipe project by running the following command: curl -s https://raw.githubusercontent.com/daipe-ai/project-creator/master/create_project.sh | bash -s skeleton-databricks Prerequisites The following software needs to be installed first: Miniconda package manager Git for Windows or standard Git in Linux ( apt-get install git ) We recommend using the following IDEs: PyCharm Community or Pro with the EnvFile plugin installed Visual Studio Code with the PYTHONPATH setter extension installed Tu run commands, use Git Bash on Windows or standard Terminal on Linux/Mac What does the command do? Asks for project & directory name of your new project Download the Daipe project skeleton template Create the new project skeleton based on the template Runs the Daipe development environment initialization script","title":"Creating from skeleton"},{"location":"creating-custom-output-decorator/","text":"Creating a custom output decorator \u00b6 We are going to create a @send_to_api decorator which sends data to an API. Inside our project's src/__project_name__/lib we create a file called send_to_api.py We must adhere to this interface. @DecoratedDecorator class send_to_api ( OutputDecorator ): def __init__ ( self , * args , ** kwargs ): # init def process_result ( self , result : DataFrame , container : ContainerInterface ): # the decorator logic The input arguments of the class are completely arbitrary. We are using an url variable to specify where to send the data. The process_result() function has a fixed interface. It recieves the result DataFrame and a container with Daipe services and configuration. All the objects necessary to process the DataFrame can be obtained from the container e. g. Logger. We define a custom method for sending data to an API. def __send_to_api ( self , df ): df_json = df . toPandas () . to_json () conn = http . client . HTTPSConnection ( self . __url ) conn . request ( \"POST\" , \"/\" , df_json , { 'Content-Type' : 'application/json' }) The complete output decorator class can look like this from logging import Logger import http.client from daipecore.decorator.DecoratedDecorator import DecoratedDecorator from daipecore.decorator.OutputDecorator import OutputDecorator from injecta.container.ContainerInterface import ContainerInterface from pyspark.sql import DataFrame @DecoratedDecorator class send_to_api ( OutputDecorator ): def __init__ ( self , url : str ): self . __url = url def process_result ( self , result : DataFrame , container : ContainerInterface ): logger : Logger = container . get ( \"datalakebundle.logger\" ) logger . info ( f \"Sending { result . count () } records to API\" ) self . __send_to_api ( result ) def __send_to_api ( self , df ): df_json = df . toPandas () . to_json () conn = http . client . HTTPSConnection ( self . __url ) conn . request ( \"POST\" , \"/\" , df_json , { 'Content-Type' : 'application/json' }) In our project we simple import the decorator using from __myproject__.lib.send_to_api import send_to_api","title":"Creating a custom output decorator"},{"location":"creating-decorator-function/","text":"Creating custom decorator function \u00b6 First let's create a folder lib inside the root of our project to contain the custom code. We are going to create a table_stream_read decorator to read a table as stream. Let's create a file called table_stream_read . Now we just need to ashere to the interface. The function must be decorated using @input_decorator_function and it must contain a definition of a wrapper function which it returns. Example code: from daipecore.function.input_decorator_function import input_decorator_function from injecta.container.ContainerInterface import ContainerInterface from pyspark.sql import SparkSession from datalakebundle.table.parameters.TableParametersManager import TableParametersManager @input_decorator_function def read_stream_table ( identifier : str ): def wrapper ( container : ContainerInterface ): table_parameters_manager : TableParametersManager = container . get ( TableParametersManager ) table_parameters = table_parameters_manager . get_or_parse ( identifier ) spark : SparkSession = container . get ( SparkSession ) return spark . readStream . format ( \"delta\" ) . table ( table_parameters . full_table_name ) return wrapper Usage \u00b6 from __myproject__.lib.read_stream_table import read_stream_table @transformation ( read_stream_table ( \"bronze.steaming_events\" )) @table_overwrite ( \"silver.tbl_loans\" ) def save ( df : DataFrame ): return df . filter ( f . col ( \"type\" ) == \"new_loan\" ) . orderBy ( \"LoanDate\" )","title":"Creating a decorator function"},{"location":"creating-pull-request/","text":"Creating a Pull Request \u00b6 When you're done with all the changes create a Pull Request from your branch to the master branch. When the Pull Request is created the CICD pipeline is automatically triggered and the Databricks notebooks and associated DataFactory Pipelines are deployed to the TEST environment. There will be newly created DataFactory instance based on the name of the feature branch, you can find the link in the DevOps Pipelines - Deploy Data Factory to test environment. To run tests: Open the TEST Data Factory Go to Author Section Select the pipeline you updated (or any other pipeline you want to testify is functional) and hit Debug After the successful run of the tests add the Reviewer to the Pull Request and provide him with the test results (for instance, by adding a comment to the PR). When it's approved the PR could be merged by Squash commit to the master and the new awesome feature is automatically deployed to the DEV and with approval to the PROD environments.","title":"Creating a Pull Request"},{"location":"customizing-table-defaults/","text":"Customizing table defaults \u00b6 Sometimes your table names may contain additional flags to explicitly emphasize some meta-information about the data stored in that particular table. Imagine that you have the following tables: customer_e.my_table product_p.another_table The e/p suffixes describe the fact that the table contains encrypted or plain data. What if we need to use that information in our code? You may always define the attribute explictly in the tables configuration: parameters : datalakebundle : table : name_template : '{identifier}' tables : customer_e.my_table : encrypted : True product_p.another_table : encrypted : False If you don't want to duplicate the configuration, try using the defaults config option to parse the encrypted/plain flag into the new encrypted boolean table attribute: parameters : datalakebundle : table : name_template : '{identifier}' defaults : encrypted : !expr 'db_identifier[-1:] == \"e\"' For more complex cases, you may also use a custom resolver to create a new table attribute: from box import Box from datalakebundle.table.identifier.ValueResolverInterface import ValueResolverInterface class TargetPathResolver ( ValueResolverInterface ): def __init__ ( self , base_path : str ): self . __base_path = base_path def resolve ( self , raw_table_config : Box ): encrypted_string = 'encrypted' if raw_table_config . encrypted is True else 'plain' return self . __base_path + '/' + raw_table_config . db_identifier + '/' + encrypted_string + '/' + raw_table_config . table_identifier + '.delta' parameters : datalakebundle : table : name_template : '{identifier}' defaults : target_path : resolver_class : 'datalakebundle.test.TargetPathResolver' resolver_arguments : - '%datalake.base_path%'","title":"Setting table defaults"},{"location":"customizing-table-names/","text":"Customizing table names \u00b6 Table naming can be customized to match your company naming conventions. By default, all tables are prefixed with the environment name (dev/test/prod/...): parameters : datalakebundle : table : name_template : '%kernel.environment%_{identifier}' The {identifier} is resolved to the table identifier defined in the datalakebundle.tables configuration (see above). By changing the name_template option, you may add some prefix or suffix to both the database, or the table names: parameters : datalakebundle : table : name_template : '%kernel.environment%_{db_identifier}.tableprefix_{table_identifier}_tablesufix'","title":"Customizing table names"},{"location":"daipe-2-0-release-notes/","text":"Daipe 2.0: Release Notes \u00b6 Enhancements \u00b6 It is no longer necessary to define tables in a local environment, YAML config is optional . Local environment is only necessary for the initial setup of the project It is now possible to use Daipe without Databricks on whatever Spark environment or even without Spark just using Pandas Functions such as read_csv() and read_table() can be used as arguments for decorators. This completely replaces the functionality of @data_frame_loader , see docs . Example: # Old Daipe @data_frame_loader ( display = True ) def my_transformation ( spark : SparkSession ): return spark . read . table ( \"my_database.my_table\" ) # New Daipe @transformation ( read_table ( \"my_database.my_table\" ), display = True ) def my_transformation ( df : DataFrame ): return df Support for DBR 8.x Decorator @table_overwrite which overwrites all data in a table with the data from a DataFrame, see docs Decorator @table_append which appends the data from a DataFrame to a table, see docs Decorator @table_upsert which updates existing data based on primary_key and inserts new data, see docs Schema now allows you to define a primary_key (used for @table_upsert ), partition_by and tbl_properties , see docs Schema will be generated for you if you do not provide it to the @table_* decorators see example: Schema checking output is greatly improved. Schema diff example: Backwards incompatible changes \u00b6 Schema is no longer loaded automatically from the schema.py file in the notebook folder. Now the schema can be defined inside the notebook as well as imported from a separate file, see docs and example: Command console datalake:table:create-missing has been removed , because it is no longer possible to rely on the tables being defined in YAML config Command console datalake:table:delete renamed to console datalake:table:delete-including-data Deprecations \u00b6 Decorator @data_frame_loader has been deprecated Decorator @data_frame_saver has been deprecated","title":"Daipe 2.0"},{"location":"data-pipelines-project-setup/","text":"Creating first Data Pipelines project \u00b6 Go to setup steps \u2193 Introduction and Prerequisites \u00b6 daipe-project-creator.yml is Azure DevOps pipeline that initiates a new Data Pipelines project from the up-to-date Daipe template . The pipeline also contains the script for automatic protection of the master branch in newly created project. Data Pipelines project consists of the following components: A git repo with DataFactory pipelines, Daipe project structure. CI/CD pipelines: for testing/deployment of your Databricks notebooks / pipelines to the Databricks workspace based on the environment for testing/deployment of DataFactory linked to your project Prerequisites : For initial setup of Data Pipelines project / repository: Successfully deployed DEV/TEST/PROD Infrastructure from Datalake resources page Permission to create Personal Access token in your DevOps organization project How to set up Data Pipelines project? \u00b6 1. Create Personal access token for the first run of the pipeline \u00b6 The Personal Access token is needed just for the first run of the pipeline to create a new Daipe based project DevOps repository and initial set up of the CICD pipeline It can be deleted after the successful run of the daipe-project-creator.yml Go to : Then create the new Personal Token with these permissions. 2. Register the Daipe project creator pipeline and run it \u00b6 Create a new DevOps pipeline based on .cicd/pipelines/daipe-project-creator located in infrastructure repo. Set a PA_TOKEN variable under the Variables by providing Personal Access Token and save the pipeline. You can now rename the pipeline, if you'd like to. Run the newly created pipeline: select the dev branch provide the name of the project you want to create. *selecting dev branch is necessary for initial setup of CICD in newly created Daipe project Pipeline will need the permission to access the service principal for getting the Databricks workspace variables. Open the run of the pipeline and under the View permit the usage of service principal. Created resources \u00b6 After successful execution, you will find the following resources : A new git repo Protected master branch the initial setup will create the master branch and deploy it to the DEV environment for merging feature branch: the Pull Request needs to be done the notebooks will be deployed to the TEST environment for testing successful run of the pipelines there needs to be in minimum ONE approval for the PR Merging is done by the Squash commit Main CI/CD pipeline created for the repo. The pipeline is called master-$yourprojectname-deployment . Update project variables \u00b6 Set SERVICE_CONNECTION_NAME variables for your newly created project in .cicd/variables/variables.yml . Commit the changes: set the name of the branch, eq. update-variables check Create pull request It will automatically create Pull request to the master branch, get it approved and merge the changes. The deployment pipeline will be executed automatically, after the merge of updated variables. You can find it running under the Pipelines tab. The project will be deployed to the DEV Databricks and pipeline will ask for permission to the service connection, as it is shown on the picture below. Next steps \u00b6 After an initial run of the pipelines, you will have codes available in the Databricks workspace corresponding to the Databricks environment. You can find the links to the corresponding Databricks workspace directly in the CICD pipeline - Deploy Daipe section. Note that if you enter the workspace for the first time, you have to launch it from the corresponding Azure resource page using Azure portal. Also the DataFactory in the specific environment will be linked to the codes in the Databricks workspace. You can find the links to the corresponding DataFactory instance directly in the CICD pipeline - Deploy Data Factory section. For workflow details see Developers workflow page.","title":"Data Pipelines project"},{"location":"data-pipelines-workflow/","text":"Data pipelines development workflow \u00b6 Our default workflow uses three environments: DEV , TEST , PROD . The feature branches can be merged to the master branch once Pull Request is approved: When the Pull Request is made, the feature branch is automatically deployed to the TEST environment and the tests are run As soon as the tests are completed successfully, the release manager can approve the Pull Request to merge the new branch to master Merging is done using the \"squash\" strategy (all changes are squashed into a single commit) Environments customization Use the allowed_environments config option in the [PROJECT_ROOT]/pyproject.toml file to set environments.","title":"Overview"},{"location":"databricks-connect-setup/","text":"Databricks Connect setup \u00b6 Although Databricks Connect is NOT required when coding in notebooks , you may find it useful when working with the datalake management commands . In the [rootpackage]/_config/bundles/databricksbundle.yaml project bundle config, add the following configuration: parameters : databricksbundle : databricks_connect : connection : address : 'https://dbc-123.cloud.databricks.com' token : 'abcd123456' cluster_id : '0416-084917-doles835' Storing tokens and other sensitive information in YAML configs is generally not a good idea. Try moving the token to your environment variables and the .env file located in the project root: parameters : databricksbundle : databricks_connect : connection : address : 'https://westeurope.azuredatabricks.net' token : '%env(DBX_TOKEN)%' cluster_id : '0416-084917-doles835' How to test Databricks connection? \u00b6 To test that your local configuration works properly, activate the virtual environment and run: $ console dbx:test-connection --env=dev The environment you want to test connection against can be changed by using the --env option.","title":"Databricks Connect setup"},{"location":"datalake-resources-setup/","text":"Spinning up Data Lake resources \u00b6 Go to setup steps \u2193 Overview of project workflow (environments / branches): Environment Branch Databricks Workspace Databricks Code branch DataFactory Resource DataFactory Pipelines Code branch APP_ENV tmp tmp DBX tmp - - - - sandbox sandbox DBX sandbox [feature branch] (optional) - - dev (if Bricskflow coding standards are used) dev dev DBX dev [feature branch] (required) ADF dev [feature branch] dev test test DBX test [feature branch] (auto-deployment with PR) ADF [feature-branch] (auto-creation with PR) [feature branch] (auto-deployment with PR) dev prod master DBX prod master (auto-deployment after tag) ADF prod master prod Infrastructure repository workflow: branch based deployment each branch represents different environment with environment specific variables master branch holds the truth and deploys prod environment resources Daipe project repository workflow: feature branches are deployed to dev environment pull requests to master branch are deployed to test environment the master branch is presented in dev environment and released to prod environment after tagging the release 1. Create repository for infrastructure and import it's code \u00b6 In Azure DevOps click on repositories Click on dropdown menu Click on New repository Name it e.g. infra Uncheck Add a README Click Create Click on Import In Clone URL fill https://github.com/DataSentics/adap-infra-template.git In Username fill aisuite@datasentics.com In Password fill the password we have provided you with (In case you don't have the password send a message to marek.chlubna@datasentics.com ) Click on Import 2. Set main infrastructure variables \u00b6 The file .cicd/variables/variables.yml holds the main variables that you can use to customize your infrastructure. The files .cicd/variables/variables-{temp/sand/dev/test/prod}.yml hold specific variables for each environment. Replace the general placeholders in .cicd/variables/variables.yml : TENANT_ID - from Azure setup section 5 PROJECT_NAME - !! should be simple lowercase name (max 5 characters) !! GIT_ACCOUNT_NAME - name of your devops organization GIT_PROJECT_NAME - name of your devops project 3. Create environment based branches \u00b6 Create branches based on environments you want to deploy: (this needs to be done for all the environments you're about to deploy) For updating environment specific variables create branch and name it after the environment you want to deploy, update environment specific variables. Non prod environment: checkout newly created branch in the file .cicd/variables/variables.yml update SERVICE_CONNECTION_NAME variable for the environment you're about to deploy update environment specific variables in the file .cicd/variables/variables-{environment}.yml update desired environment variables here change ADMIN_OBJECT_ID to object id of user of your choice. This user will have admin access to created keyvault You can find user object id in Active Directory. After the successful deployment based on next steps merge the commits to the master branch! Prod environment: Prod environment is based on the master branch. When you're about to deploy prod resources, updated the prod based variables through the pull request, optional directly in the master branch. 3. Create DevOps pipeline for infrastructure build & deployment \u00b6 In Azure DevOps click on pipelines Click on New pipeline Select Azure Repos Git Select infra repository It will automaticaly locate file azure-pipelines.yml Click on Save Click on run and select created branch with the variables of the environment you'd like to deploy. The environment resource group based on selected branch is deployed to the Subscription. Run the pipeline again with different branch selected if you'd like to deploy another environment. 4. Create Key Vault Secret Scope in Databricks \u00b6 When the pipeline is finished you need to create secret scope for Databricks. !! This needs to be done for all environments you deployed {temp/sand/dev/test/prod} !! Go to Databricks workspace Look in the URL There should be something like https://adb-3076017168624144.4.azuredatabricks.net/?o=3076017168624144 Add #secrets/createScope at the end of URL URL now should look like https://adb-3076017168624144.4.azuredatabricks.net/?o=3076017168624144#secrets/createScope Hit enter and you should be redirected to the page below Fill in information Scope Name - unit-kv DNS Name and Resource ID can be found in key vault properties 5. Resources overview \u00b6 After the infrastructure is deployed you can check the resources under resource group adap-cz-PROJECT_NAME-rg-dev Main components Databricks workspace - this is place where you develop your spark notebooks Storage accoount - this is place where your data lives Key vault - this is place where secrets are stored Data factory - main orchestration engine for your Databricks notebooks Virtual network - Key vault and Databricks clusters are deployed in this virtual network for better isolation","title":"Data Lake resources"},{"location":"datalake-storage-path/","text":"Setting datalake storage path \u00b6 Add the following configuration to config.yaml to set the default storage path for all the datalake tables: parameters : datalakebundle : defaults : target_path : '/mybase/data/{db_identifier}/{table_identifier}.delta' When setting defaults , you can utilize any of the following placeholders: {identifier} - customer.my_table {db_identifier} - customer {table_identifier} - my_table parsed custom fields How to set storage path for a specific table? Storage path of any specific table can be easily changed by adding the target_path attribute to given table's configuration: parameters : datalakebundle : tables : customer.my_table : target_path : '/some_custom_base/{db_identifier}/{table_identifier}.delta'","title":"Setting datalake storage path"},{"location":"datalake-structure/","text":"How does a datalake look like? \u00b6 It is recommended to structure your tables into the following layers: Detailed description bronze - \"staging layer\", raw data from source systems silver - most business logic, one or multiple tables per use-case parsed - data loaded from bronze layer to be stored as datalake tables cleansed - data cleaning, source systems bug fixing, ... Smart Data Model (SDM) - fully prepared data for further analytical and machine-learning use-cases gold - additional filtering/aggregations of silver data (using views or materialized tables) to be served to the final customers. Feature Store - central place where customer/product/... features are stored, managed and governed within the organization. reporting marts - pre-aggregated views of the data suitable for reporting and visualizations.","title":"How does a datalake look like?"},{"location":"decorator-functions/","text":"Decorator functions \u00b6 read_csv \u00b6 read_csv ( path: str, schema: StructType = None, options: dict = None ) Reads a CSV file into a spark DataFrame Parameters: path : str - path to the CSV file schema : StructType, default None - schema of the CSV file options : dict, default None - options passed to spark.read.options(**options) Example: @transformation ( read_csv ( \"/LoanData.csv\" , options = dict ( header = True , inferSchema = True )), display = True ) @table_overwrite ( \"bronze.tbl_loans\" ) def save ( df : DataFrame ): return df . orderBy ( \"LoanDate\" ) read_delta \u00b6 read_delta ( path: str, schema: StructType = None, options: dict = None ) Reads a Delta from a path Parameters: path : str - path to the Delta schema : StructType, default None - Union[str, list], default None - schema of the Delta options : dict, default None - options passed to spark.read.options(**options) read_json \u00b6 read_json ( path: str, schema: StructType = None, options: dict = None ) Reads a json file from a path Parameters: path : str - path to the json file schema : StructType, default None - Union[str, list], default None - schema of the json file options : dict, default None - options passed to spark.read.options(**options) read_parquet \u00b6 read_parquet ( path: str, schema: StructType = None, options: dict = None ) Reads a parquet from a path Parameters: path : str - path to the parquet schema : StructType, default None - Union[str, list], default None - schema of the parquet options : dict, default None - options passed to spark.read.options(**options) read_table \u00b6 read_table ( identifier: str ) Reads a table into a spark DataFrame Parameters: identifier : str - full table name, format db.table_name Example: @transformation ( read_table ( \"silver.tbl_loans\" )) def read_table_bronze_loans_tbl_loans ( df : DataFrame , dbutils : DBUtils ): base_year = dbutils . widgets . get ( \"base_year\" ) return df . filter ( f . col ( \"DefaultDate\" ) >= base_year ) table_params \u00b6 table_params ( identifier: str, param_path_parts: list = None ) Reads parameters from datalakebundle.tables.[ identifier ] Parameters: identifier : str - full table name, format db.table_name param_path_parts : list, default None - Union[str, list], default None - list of parameter levels leading to result","title":"Decorator functions"},{"location":"deploy-demo-project/","text":"Deploy to Databricks \u00b6 Now you can push your project to Databricks Workspace using following command $ console dbx:deploy And you should see the pushed project in Databricks workspace into the Environment Variables . Important scripts: poe flake8 - checks coding standards poe container-check - check app container consistency (if configured properly)","title":"Deploying to Databricks"},{"location":"devops-agents-setup/","text":"Setting up Azure DevOps agents (optional) \u00b6 You need some compute where your CI/CD pipelines are run. You can use Microsoft hosted agents or if you want more control over your CI/CD runtime you can setup self-hosted agents. 1. Create Azure DevOps Agent Pool \u00b6 First you need to create Agent Pool in Azure DevOps. Go to your project in Azure DevOps Click on Settings Click on Agent pools Click on Add pool Select Self-hosted Pool type Fill ADAP Pool in name Uncheck Grant access permission to all pipelines Click Create 2. Create Personal Access Token (PAT) \u00b6 Now we need to create Personal Access Token that will have access to this pool. Click on User setting in upper right corner and then on Personal access tokens Click on New Token In right menu click on Show all scopes Select Agent Pools Read & Manage In Name fill AZP_TOKEN Click Create Copy and store the token for later use 3. Create Container Registry \u00b6 We need registry to store our docker image for agents. In Azure DevOps click on repositories Click on dropdown menu Click on New repository Name it e.g. container-registry Uncheck Add a README Click Create Click on Import In Clone URL fill https://github.com/DataSentics/adap-container-registry-template.git In Username fill your Datasentics email In Password fill your Github password / token Click on Import After import is done open file variables.yml Replace placeholders SERVICE_CONNECTION - your dev service connection, e.g. devops-service-connection-to-devsubscription ACR_NAME - name of the registry, e.g. mydevopsregistry Caution Registry name must be globally unique and may contain only lower case letters up to 23 characters. Create pipeline to deploy your registry In Azure DevOps click on pipelines Click on New pipeline Select Azure Repos Git Select container-registry repository Click Run This pipeline will deploy container registry resource. 4. Create Agents \u00b6 Now we need to deploy the agents it self. Repeat steps from previous section to create new repository but this time name the repository e.g. devops-agents and import code from https://github.com/DataSentics/adap-devops-agents-template.git After import is done open file variables.yml and replace placeholders SERVICE_CONNECTION - your dev service connection, e.g. devops-service-connection-to-devsubscription ACR_NAME - name of your previously created registry ( mydevopsregistry ) DEVOPS_URL - url of your devops organization, e.g. https://dev.azure.com/organization/ You can also set how much agents you want (AGENTS_COUNT) and how much cpu and memory in GB should each agent have (AGENT_CPU, AGENT_MEMORY) Repeat steps from previous section to create pipeline but this time select devops-agents repository Click on Variables Click on New variable In Name fill DEVOPS_PAT In Value fill token from section 2 Check Keep this value secret Click OK Click Save Click Run After few minutes when the pipeline finishes you should be able to see agent (or agents) registered in ADAP Pool 5. Let pipelines know to use this Agent Pool \u00b6 So far we only created Agent Pool and registered agent(s) in that pool. But in order to run jobs/tasks on that pool we must inform our pipelines to use that pool. Our pipelines are defined in repository as a yaml code Lets take look at infra repo created in section Data Lake resources Open file azure-pipelines.yml and you should see following code You can see following code at lines 17 and 18 pool: vmImage: 'ubuntu-20.04' This needs to be replaced for pool: 'ADAP Pool' You also need to make same replacement in file .cicd/templates/release-jobs.yml When You commit the changes next time you run infra pipeline it will run on self-hosted agent registered in ADAP Pool","title":"Self-hosted agents (optional)"},{"location":"input-decorators/","text":"Input decorators \u00b6 These decorators are used to wrap the entire content of a cell. @transformation \u00b6 @transformation ( *objects, display = False, check_duplicate_columns = True ) Used for decorating a function which manipulates with a DataFrame. Runs the decorated function upon declaration. *objects : an arbitrary number of objects passed to the decorated function display : bool, default False - if True the output DataFrame is displayed check_duplicate_columns : bool, default True - if True raises an Exception if there are duplicate columns in the DataFrame Example: @transformation ( read_table ( \"silver.tbl_loans\" ), read_table ( \"silver.tbl_repayments\" ), display = True ) @table_overwrite ( \"silver.tbl_joined_loans_and_repayments\" , get_joined_schema ()) def join_loans_and_repayments ( df1 : DataFrame , df2 : DataFrame ): return df1 . join ( df2 , \"LoanID\" ) @notebook_function \u00b6 @notebook_function ( *objects ) Used for decorating any other function which is not decorated with the @transformation decorator. Runs the decorated function upon declaration. Parameters: *objects : an arbitrary number of objects passed to the decorated function display : bool, default False - if True the output DataFrame is displayed check_duplicate_columns : bool, default True - if True raises an Exception if there are duplicate columns in the DataFrame Example: @notebook_function () def download_data (): opener = urllib . request . URLopener () opener . addheader ( \"User-Agent\" , \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" , ) opener . retrieve ( \"https://www.bondora.com/marketing/media/LoanData.zip\" , \"/loanData.zip\" ) opener . retrieve ( \"https://www.bondora.com/marketing/media/RepaymentsData.zip\" , \"/repaymentsData.zip\" ) Objects available in @transformation and @notebook_function \u00b6 spark: SparkSession dbutils: DBUtils logger: Logger Using Spark and Logger from logging import Logger from pyspark.sql.session import SparkSession @notebook_function () def customers_table ( spark : SparkSession , logger : Logger ): logger . info ( 'Reading my_crm.customers' ) return spark . read . table ( 'my_crm.customers' ) Using DBUtils from pyspark.dbutils import DBUtils @notebook_function () def create_input_widgets ( dbutils : DBUtils ): dbutils . widgets . dropdown ( \"base_year\" , \"2015\" , list ( map ( str , range ( 2009 , 2022 ))), \"Base year\" )","title":"Input decorators"},{"location":"local-machine-limited-access/","text":"Local machine has limited internet access \u00b6 In this case we assume that local machine has limited internet access access, e.g. can't do pip install package . For this case we have prepared repository with files and scripts that support this situation. We use basic python virtual environment instead of conda, because conda needs to reach internet when creating virtual env. You can get this repository at https://github.com/daipe-ai/offline-access.git and copy following files/directories to your daipe project. dependencies/ - directory where all the dependencies (.whl files) are stored so we don't have to install them from PyPI .poetry/ - directory where poetry package manager is stored so we don't have to install it from internet env-init-offline.sh - offline environment initialization script activate.sh - environment activation script deactivate.sh - environment deactivation script azure-pipelines.yml - simple offline devops pipeline You can now do ./env-init-offline.sh which will initialize your local environment without touching the internet. You can activate/deactivate environment using following commands source activate.sh source deactivate.sh After activating virtual environment you should be able to run standard Daipe commands like console dbx:deploy . Note In dependencies directory we included dependencies for Windows/Linux and Python 3.7. So we are able to develop Daipe project on local Windows machine and also use it on some ci/cd Linux agent. Also very important thing is that our target Databricks runtime is DBR 7.3 which is Linux with Python 3.7. If you want to make some changes, e.g. add some python package it's your responsibility to add appropriate wheels in the dependencies/ direcotry. Edge case One edge case we run into in one very strict environment is that we were not able to run console command because it is an executable and only defined set of executables was allowed to run. To avoid this issue we can run console command in this way python .venv/Lib/site-packages/consolebundle/CommandRunner.py dbx:deploy . To make life easier we can add following line in the .bashrc - alias console='python .venv/Lib/site-packages/consolebundle/CommandRunner.py' . Be aware that this way the console command will work only from project root. How to get dependencies \u00b6 Databricks dependencies To get dependencies that are needed to run application on databricks you can use command console dbx:build-dependencies as it was documented in first section of this page. Note Dependencies built with console dbx:build-dependencies are just dependencies that are needed to run application itself excluding development dependencies like flake8 etc. If you also want to build development dependecies you can pass --dev flag. Dependencies built this way should be runable on most linux platforms with appropriate python version that was used in Databricks runtime. Local dependencies To get local dependencies that are specific to your platform you can use this sequence of commands. poetry export --dev --without-hashes -o dev-requirements.txt python -m pip wheel -r dev-requirements.txt -w dependencies/ rm dev-requirements.txt","title":"Local machine access limited"},{"location":"managing-tables-console/","text":"Managing datalake tables using console commands \u00b6 Example : To create the customer.my_table table in your datalake, just type console datalake:table:create customer.my_table --env=dev into your terminal within your activated project's virtual environment. The command connects to your cluster via Databricks Connect and creates the table as configured. Datalake management commands: datalake:table:create [table identifier] [path to TableSchema module] - Creates a metastore table based on it's YAML definition (name, schema, data path, ...) datalake:table:recreate [table identifier] [path to TableSchema module] - Re-creates a metastore table based on it's YAML definition (name, schema, data path, ...) datalake:table:delete-including-data [table identifier] - Deletes a metastore table including data on HDFS datalake:table:optimize [table identifier] - Runs the OPTIMIZE command on a given table","title":"Managing tables using console"},{"location":"new-databricks-notebook/","text":"Creating a new notebook \u00b6 Cloning the master branch/folder Open the Dev Databricks Workspace associated with your project Under the Workspace, find the folder that has the name of your project When you open it, you can see the branches active under your project Clone the master folder and make your branch folder out of it By cloning the master you will have the most updated version of the code on your branch folder. Name it feature-<your-branch-name> . Then you can start working on the awesome new feature in your separate branch. When you're done with coding, you can contact Data Engineer to commit the changes for you.","title":"Creating a new notebook"},{"location":"new-datafactory-pipeline/","text":"Setting up a DataFactory pipeline \u00b6 Go to the Data Factory associated with your project. You can find it under the Dev Resource Group Or you can open the link to the instance in DevOps Pipelines - Deploy Data Factory to dev environment (see picture below). In Data Factory go to the Author section. Create or Select existing branch: If you are creating pipeline for the notebooks in the master branch select Create new and name it If you want to create pipeline based on the notebooks from the commited feature branch select Use existing and select the branch On top you can see that you are using your branch. You can create a new pipeline or update the existing one by selecting it under the Pipelines. We are going to update the existing one CovidPipeline for our demo purpose. When you're done with editing the pipeline: Debug it After successful run hit Save (or Save All for multiple pipelines) The new commit in DevOps repository under your branch used in Data Factory will be created.","title":"Setting up a pipeline"},{"location":"offline-databricks-solution/","text":"Databricks clusters don't have internet access \u00b6 This section is maninly for those who are working in some strict environment where you may not have full internet access. However there are couple of assumptions Git for Windows installed Python 3.7 or Conda installed Ability to clone or download Github repository Ability to call Databricks API Databricks Personal Access Token This is one of the most common cases. Daipe framework works on your local machine as usual but Databricks clusters can't reach internet, hence cell %run install_master_package is failing. Daipe framework has built in feature to solve this situation. First you need to build dependencies (python wheel packages) needed by master package. You can check what dependencies are needed by running poetry export --without-hashes . Then you need to add those dependencies (.whl files) to dependencies/ directory at the project root. You can do this process by your self by downloading packages manually or building them on some linux based os/docker. Be aware that if you are downloading the packages manually you need to make sure that You are downloading exact version that is needed You are downloading linux compatible package according to Databricks runtime You are downloading python compatible package according to Databricks runtime python version Or you can use automated way which daipe framework offers. In src/ myproject /_config/bundles/dbxdeploy.yaml you can configure on which Databricks workspace and runtime you want to build packages. parameters : dbxdeploy : target : package : build : databricks : host : '%dbxdeploy.databricks.host%' token : '%dbxdeploy.databricks.token%' job_cluster_definition : spark_version : '7.3.x-scala2.12' node_type_id : 'Standard_DS3_v2' num_workers : 1 Note that to build the dependencies the cluster must have internet access. We assume that if you want to build dependecies with ease you will have some non-productional Databricks workspace with internet access. When your config is setup you can just use command console dbx:build-dependencies and daipe will build dependencies on that cluster for you and automatically downloads them in dependencies/ directory. Now to deploy project in the way that %run install_master_package cell doesn't need to reach internet you need to set offline_install: True in dbxdeploy config. parameters : dbxdeploy : target : package : offline_install : True Finally you can use console dbx:deploy to deploy your project.","title":"No internet access on Databricks"},{"location":"output-decorators/","text":"Output decorators \u00b6 Output decorators are used to persist the output of the decorated function in multiple possible formats - table, delta, csv, json and parquet. @table_overwrite \u00b6 @table_overwrite ( identifier: str, table_schema: TableSchema = None, recreate_table: bool = False, options: dict = None ) Overwrites data in a table with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema recreate_table : bool, default False, if True the table is dropped if exists before written to options : dict, default None - options which are passed to df.write.options(**options) @table_append \u00b6 @table_append ( identifier: str, table_schema: TableSchema = None, options: dict = None ) Appends data to a table with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema options : dict, default None - options which are passed to df.write.options(**options) @table_upsert \u00b6 @table_upsert ( identifier: str, table_schema: TableSchema ) Updates data or inserts new data to a table based on primary key with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema @csv_append \u00b6 @csv_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a CSV file Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @csv_overwrite \u00b6 @csv_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a CSV file by a spark DataFrame Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @csv_write_ignore \u00b6 @csv_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a CSV file if it does not exist Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @csv_write_errorifexists \u00b6 @csv_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a CSV file, throws an Exception if it already exists Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @delta_append \u00b6 @delta_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a Delta Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @delta_overwrite \u00b6 @delta_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a Delta by a spark DataFrame Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @delta_write_ignore \u00b6 @delta_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a Delta if it does not exist Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @delta_write_errorifexists \u00b6 @delta_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a Delta, throws an Exception if it already exists Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @json_append \u00b6 @json_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a json file Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @json_overwrite \u00b6 @json_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a json file by a spark DataFrame Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @json_write_ignore \u00b6 @json_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a json file if it does not exist Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @json_write_errorifexists \u00b6 @json_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a json file, throws an Exception if it already exists Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @parquet_append \u00b6 @parquet_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a parquet file Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @parquet_overwrite \u00b6 @parquet_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a parquet file by a spark DataFrame Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @parquet_write_ignore \u00b6 @parquet_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a parquet file if it does not exist Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @parquet_write_errorifexists \u00b6 @parquet_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a parquet, throws an Exception if it already exists Parameters: path : str - path to the parquet partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options)","title":"Output decorators"},{"location":"project-code-structure/","text":"Recommended project code structure \u00b6 For databases and tables in each of bronze/silver/gold layers it is recommended to follow the [db_name/table_name] directory structure: [ PROJECT_ROOT ] /src [ROOT_MODULE_NAME] # most commonly name of your company or product bronze_db_batch tbl_customers.py tbl_products.py tbl_contracts # it is possible to place notebooks in folders with the same name if necessary tbl_contracts.py csv_schema.py ... silver_db_batch tbl_product_profitability.py tbl_customer_profitability.py tbl_customer_onboarding.py ... gold_db_batch vw_product_profitability.py # view on silver_db_batch.tbl_product_profitability tbl_customer_profitability.py # \"materialized\" view on silver_db_batch.tbl_customer_profitability vw_customer_onboarding.py","title":"Project code structure"},{"location":"setting-table-specific-params/","text":"Setting table-specific configuration \u00b6 Besides the basic configuration options , you can also define configuration for specific datalake tables : parameters : datalakebundle : tables : customer.my_table : params : test_data_path : '/foo/bar' Code of the customer/my_table.py notebook: from logging import Logger from datalakebundle.notebook.decorators import notebook_function , table_params @notebook_function ( table_params ( 'customer.my_table' ) . test_data_path ) def customers_table ( test_data_path : str , logger : Logger ): logger . info ( f 'Test data path: { test_data_path } ' ) The table_params('customer.my_table') function call is a shortcut to using %datalakebundle.tables.\"customer.my_table\".params% string parameter.","title":"Setting table-specific parameters"},{"location":"table-schema/","text":"TableSchema \u00b6 TableSchema ( fields: list, primary_key: Union[str, list] = None, partition_by: Union[str, list] = None, tbl_properties: dict = None ) Defines a table schema Parameters: fields : list - list of StructField defining columns of the table primary_key : Union[str, list], default None - primary key or a list of keys used for @table_upsert partition_by : Union[str, list], default None - one or multiple fields to partition the data by, optional tbl_properties : dict, default None - key value pairs to be added to TBLPROPERTIES , optional Example: def get_schema (): return TableSchema ( [ t . StructField ( \"ReportAsOfEOD\" , t . DateType (), True ), t . StructField ( \"LoanID\" , t . StringType (), True ), t . StructField ( \"Date\" , t . DateType (), True ), t . StructField ( \"PrincipalRepayment\" , t . DoubleType (), True ), t . StructField ( \"InterestRepayment\" , t . DoubleType (), True ), t . StructField ( \"LateFeesRepayment\" , t . DoubleType (), True ), ], primary_key = [ \"LoanID\" , \"Date\" ], partition_by = \"Date\" , tbl_properties = { \"delta.enableChangeDataFeed\" = \"true\" , } )","title":"Table schema"},{"location":"using-explicit-schema/","text":"Using explicit table schema \u00b6 Table schema can be easily created using the TableSchema class: def get_schema (): return TableSchema ( [ t . StructField ( \"ReportAsOfEOD\" , t . DateType (), True ), t . StructField ( \"LoanID\" , t . StringType (), True ), t . StructField ( \"Date\" , t . DateType (), True ), t . StructField ( \"PrincipalRepayment\" , t . DoubleType (), True ), t . StructField ( \"InterestRepayment\" , t . DoubleType (), True ), t . StructField ( \"LateFeesRepayment\" , t . DoubleType (), True ), ], primary_key = [ \"LoanID\" , \"Date\" ], # partition_by = \"Date\" ) For more details see the TableSchema reference . Selecting all fields from the schema before writing them into table: \u00b6 @transformation ( read_csv ( \"loans.csv\" )) @table_overwrite ( \"bronze.tbl_loans\" , get_schema ()) def save ( df : DataFrame ): return ( df . select ( get_schema () . fieldNames ()) ) Schema autosuggestion \u00b6 When using @table_* decorators without an explicit schema,... @transformation ( read_csv ( \"/RepaymentsData.csv\" , options = dict ( header = True )), ) @table_overwrite ( \"bronze.tbl_repayments\" ) def load_csv_and_save ( df : DataFrame ): return df ...Daipe raises a warning and generates a schema based on the DataFrame for you. Schema checking \u00b6 When using @table_* decorators with an explicit schema, Daipe checks if the schemas match and raises an Exception if they do not. It also shows a difference between the schemas so you can easily fix the problems.","title":"Using explicit schema"},{"location":"using-notebook-function/","text":"Using the @notebook_function() decorator \u00b6 The @notebook_function() decorator is very simple. It recieves any number of arguments, passes it down to its decorated function and runs the function. You can use it for example on a function which downloads data and you want to log its progress. @notebook_function () def download_data ( logger : Logger ): opener = urllib . request . URLopener () opener . addheader ( \"User-Agent\" , \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" , ) opener . retrieve ( \"https://www.bondora.com/marketing/media/LoanData.zip\" , \"/loanData.zip\" ) logger . info ( \"Loans data successfully downloaded\" ) opener . retrieve ( \"https://www.bondora.com/marketing/media/RepaymentsData.zip\" , \"/repaymentsData.zip\" ) logger . info ( \"Repayments data successfully downloaded\" ) Technical Reference Check the Technical reference for more details about the @notebook_function and other decorators.","title":"Using @notebook_function"},{"location":"using-transformation/","text":"Using the @transformation() decorator \u00b6 There are two main decorators in the Daipe framework - @transformation() and @notebook_function() . @transformation() understands Spark dataframes better and provides you with extra Spark-related functionality like display and duplicate columns checking. @notebook_function() should be used for functions and procedures which don't manipulate with a DataFrame e. g. downloading data. First, import everything necessary for a Daipe pipeline workflow: from datalakebundle.imports import * Any decorator can also take functions as parameters: @transformation ( read_csv ( \"/data.csv\" , options = dict ( header = True , inferSchema = True ) ), ) def read_csv ( df : DataFrame ): return df See the list of all functions which can be used. display=True option can be used for displaying the DataFrame. @transformation ( read_table ( \"bronze.tbl_customers\" , options = dict ( header = True , inferSchema = True ) ), display = True ) def read_tbl_customers ( df : DataFrame ): return df For more information see the technical reference . Environments \u00b6 Each table is prefixed with an environment tag ( dev , test , prod ) to separate production data from the developement code and vice versa. The Daipe framework automatically inserts the prefix based on your selected environment therefore the code stays the same across all environments.","title":"Using @transformation"},{"location":"working-with-git/","text":"Working with GIT \u00b6 Create feature branch and work on it \u00b6 One of the most common cases is that you want to create feature branch out of master and make some changes. First activate conda environment $ ca Now pull latest changes from master branch $ git pull origin master Then create a feature branch $ git checkout -b feature-new-great-stuff Finally upload your feature branch to Databricks workspace using following command $ console dbx:deploy --env = dev Your feature branch will be deployed to the DEV Databricks workspace. You can now code some awesome new features right in Databricks workspace! Commit your work to GIT repository \u00b6 Once you're happy with what you've done in your feature branch you probably want to commit and push your changes to git repository. First download all of your work in Databricks workspace to your local machine using following command $ console dbx:workspace:export --env = dev Now you can commit and push your changes to repository $ git add . $ git commit -m \"Awesome new feature\" $ git push origin feature-new-great-stuff Getting in sync with same with existing feature branch folder in Databricks \u00b6 If you are Data Engineer and Data Scientist provided the Databricks folder for you: Create a feature branch on local with the same name as the folder in databricks. Use console dbx:workspace:export to sync the notebooks to the local commit the changes and push them to git service (GitHub, Devops, ...) Updating The Master Package \u00b6 $ console dbx:deploy-master-package --env = dev You have certainly noticed, that in the local Daipe project there is a way more code than in Databricks, where only notebooks exists. It's mainly some yaml and toml configuration files which are uploaded in master package which is installed in each notebook. So if we want to make change to those files our only option is to edit them locally. Example: Adding or updating a project dependency \u00b6 Adding a dependency \u00b6 Suppose we are developing some notebook in Databricks and now we need some new python package (dependency), e.g. scipy . In order to do that we need to follow a series of steps. Add it to pyproject.toml , build master package again and upload it to Databricks. To add scipy to pyproject.toml we need to run poetry add scipy Now we don't want to do console dbx:deploy because it would overwrite our notebooks in Databricks. Instead we want to only update master package. To do that you can use command console dbx:deploy-master-package Now we run %run install_master_package cell again. Now you should be able to import scipy module Important: The updated project should then be pushed to a central repository so that other team members can pull it and have the same dependencies. The dependencies are installed automatically after running git pull Updating a dependency \u00b6 Let's assume that we want to update a depency, e. g. datalake-bundle . We need to follow a series of steps similar to the previous case. We need to check pyproject.toml that the dependency has the correct version defined e. g. datalake-bundle = \"^1.0\" will only update to versions 1.* but not to 2.* or higher. Then we run poetry update datalake-bundle . DO NOT run poetry update without an argument. It updates all packages which might break the entire project. For now on we follow the steps 3 - 6 from the previous example. We use command console dbx:deploy-master-package Now run %run install_master_package cell again. Now you should be able to use the updated datalake-bundle module Important: The updated project should then be pushed to a central repository so that other team members can pull it and have the same dependencies. The dependencies are installed automatically after running git pull","title":"Working with git"},{"location":"workspace-cluster-connection-setup/","text":"Workspace & cluster connection setup \u00b6 Once the environment setup is completed, finish the project setup by : Putting your Databricks workspace address into the src/[ROOT_MODULE]/_config/config_dev.yaml file. Setting the DBX_TOKEN varible in the [PROJECT_ROOT]/.env file with your Databricks personal access token. Activating the Conda environment by running conda activate $PWD /.venv # or use the `ca` alias Deploying your new project to databricks workspace by running $ console dbx:deploy --env = dev Now you should your new project in the configured Databricks workspace:","title":"Connection setup"},{"location":"writing-function-output/","text":"Writing function output \u00b6 Output decorators are used in conjunction with the @transformation input decorator. They take either a string identifier of the table... @transformation ( read_csv , display = True ) @table_overwrite ( \"bronze.tbl_customers\" ) def save ( df : DataFrame , logger : Logger ): logger . info ( f \"Saving { df . count () } records\" ) return df . withColumn ( \"Birthdate\" , f . to_date ( f . col ( \"Birthdate\" ), \"yyyy-MM-dd\" )) ...or you also skip Hive and write directly into delta: @transformation ( read_table ( \"bronze.tbl_customers\" ), read_table ( \"bronze.tbl_contracts\" )) @delta_overwrite ( \"/path/to/dataset.delta\" ) def join_tables ( df1 : DataFrame , df2 : DataFrame ): return df1 . join ( df2 , \"Customer_ID\" ) For more details see the Output decorators reference . Automatic schema \u00b6 When using the string table identifier, the @table_overwrite decorator saves the data using the DataFrame schema. This is useful for prototyping. It is highly recommended to use explicit schema for production pipelines.","title":"Writing function output"},{"location":"settle-dq/","text":"Settle DQ \u00b6 Data users trust their scientists and engineers that they deliver reliable data results.\u200b Settle DQ is a data quality solution that enables Data Users and Data Engineers to understand each other and continuously pursue data quality in a way that is relevant to both groups. Settle DQ is integrated with the rest of the AI Platform, but it can be used on its own. Getting started A guide on how to set up Settle DQ on Azure Developing DQ Expectations A guide on how Settle DQ works and how to use it.","title":"Overview"},{"location":"settle-dq/developing-expectations/","text":"Settle DQ Workflow \u00b6 Working with Settle DQ is a continuous process. The Data User and the Data Engineer settle on DQ Agreements - a short description of condtions that the data should satisfy. The Data Engineer then develops DQ Expectations - declarative definitions that check whether data satisfy the given conditions. Then the Data Engineer creates validation notebooks that validate the data against the expectations (this only needs to be done for new checked data sources). The Data User then sees the validation results in a web app every time new data are processed. When conditions change, Data User and the Data Engineer discuss to update and/or create new DQ Agreements .","title":"Workflow Overview"},{"location":"settle-dq/developing-expectations/develop-store/","text":"Developing and Storing DQ Expectations \u00b6 After the Data Engineer settles with the Data User on DQ Agreements, it's time for them to develop expectations. You can think of this process as if you're using a command-line - you write some code that defines expectations and save them to the store. You only run this code once and then you can throw it away. When you want to update your expectations, you don't change your code, but write a different piece of code that updates the expectations in the store. First, you need to run the Configuration Notebook % run ./ path / to / conf_notebook After that, there is a dq_tool variable available to work with the DQ Tool. To develop expectations, you need a playground - an object that is used to define and try out expectations on top of a given metastore table. my_table = 'my_db.customers' playground = dq_tool . get_playground ( table_name = my_table ) By default the playground executes expectations on a limited number of rows. If you want to increase the limit, pass the row_count_limit parameter to get_playground . If you want to use all rows, pass None as a value. An expectation describes a condition that needs to be met by data in a Hive Metastore table. There's a long list of expectation types available. You can also see all available expectations by running playground.list_available_expectation_types() Each expectation type is a method on the playground object. To define an expectation meaning There are at least 400 rows in the table , you execute the following function: line_count = my_playground . expect_table_row_count_to_be_between ( min_value = 9500 , max_value = 11000 ) line_count The moment you execute the function, the expectation is run on the given metastore table. You can see the result and iterate until you reach the definition you're satisfied with. Expectation store is a database containing your expectation definitions. When you're happy with your expectation, add it to the store: dq_tool . expectation_store . add ( expectation = line_count , table_name = my_table , severity = 'error' , agreement = 'The customer count in a dataset exported to the marketing tool should be between 9500 and 11000.' ) The severity parameter defines how serious damage there will be, should the expectation fail. Use 'error' and 'warning' values. The agreement parameter is a text of the agreement you settled upon with the Data User . It should be clear and concise and it should contain explicit values. There's no automatic sync between the agreement and the rest of the definition, so you need to make sure these are consistent. Continue adding expectations by repeating the steps above. The scope of what you can check using expectations is intentionally limited. If you want do some heavy lifting, like computing aggregations, joining different tables, you should do these in a data pipeline, store the results in a table and add an expectation on top of this table.","title":"Developing and Storing DQ Expectations"},{"location":"settle-dq/developing-expectations/edit/","text":"Editing Expectations \u00b6 After you have developed and stored your expectations, you need to make sure these stay up to date. Business requirements change all the time, Data Quality is a continuous process, rather than a one-time project. Think of this process as if you're using a command-line - you write some code that edits expectations and save them to the store. You only run this code once and then you can throw it away. First, you need to run the Configuration Notebook % run ./ path / to / conf_notebook After that, there is a dq_tool variable available to work with the DQ Tool. First, list expectations that are defined for a given table. my_table = 'my_db.customers' dq_tool . expectation_store . print ( table_name = my_table ) database_name: my_db table_name: customers suite_key: default table_expectations: - 1: expect_table_row_count_to_be_between(min_value=9500, max_value=11000) column_expectations: symbol: - 2: expect_column_values_to_not_be_null(column='symbol', mostly=0.99) - 3: expect_column_value_lengths_to_be_between(column='symbol', max_value=3, min_value=1) Each expectation has its expectation_id a number that is unique for a given table (and suite key). You can get an expectation by this number: line_count = dq_tool . expectation_store . get ( table_name = my_table_name , expectation_id = 1 ) print ( line_count ) expect_table_row_count_to_be_between(min_value=9500, max_value=11000) Let's say you acquired some more customers and need to update the expectation. line_count . kwargs [ 'min_value' ] = 10500 line_count . kwargs [ 'max_value' ] = 12000 Then you can run this expectation using a playground object: playground = dq_tool . get_playground ( table_name = my_table ) playground . run_expectation ( line_count ) When you're satisfied with the new definition, you can save it to the store. line_count . agreement = 'The customer count in a dataset exported to the marketing tool should be between 10500 and 12000.' dq_tool . expectation_store . update ( line_count ) Now your updated expectations are used in data validations. You don't need to make any changes to the validation notebook(s) You can also delete an expectation from the store by its id: dq_tool . expectation_store . remove ( table_name = table_name , expectation_id = 3 )","title":"Editing Expectations"},{"location":"settle-dq/developing-expectations/expressions/","text":"Expectations with Expressions \u00b6 When you want to do something slightly custom, but don't want to write custom expectations. Expressions in expectations are used when you want to check something custom, but the logic is not far from an expectation that is already there. Chosen expectations have their expression versions - these work the same as the ordinary ones, the only difference is that instead of a column name, you provide a column_expression . These expressions must be valid spark SQL expressions. Pretty much anything you can write after SELECT in spark SQL can be used as an expression, see the list of functions in SQL spark . The following expression groups are currently supported: single-column expressions : The expression name contains 'expression' behind 'column'. E.g. expect_column_expression_values_to_be_between is an expression version of expect_column_values_to_be_between . Instead of the column parameter, use column_expression . column-pair expressions : E.g. expect_column_expression_pair_values_to_be_equal is an expression version of expect_column_pair_values_to_be_equal . The parameters are named column_expression_A and column_expression_B multi-column expressions : E.g. expect_multicolumn_expression_values_to_be_unique is an expression version of expect_multicolumn_values_to_be_unique . Instead of the column_list parameter use column_expression_list . Warning: Don't go crazy with expressions. If it's too complicated it should probably be a spark transformation with results written to your datalake. DQ Tool should then be used to only check the results of these transformations, not a place where these transformations are defined. If your data analysts aren't well versed in SQL, or you want to hide the logic behind your expectation and keep it at one place, consider using custom expectations. Examples \u00b6 from dq_tool import DQTool dq_tool = DQTool ( spark = spark ) playground = dq_tool . get_playground ( my_df ) See what expression expectations are available: playground . list_available_expectation_with_expersion_types () A single-column expectation \u00b6 In this example we define an expectation: Difference between open and close is smaller or equal 10 , 95% of the time. The definition is as follows. Note you can use all standard parameters of the standard expectation, like mostly . open_close_diff = playground . expect_column_expression_values_to_be_between ( column_expression = 'abs(open - close)' , max_value = 10 , mostly = 0.95 ) print ( open_close_diff ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 8, \"unexpected_percent\": 0.4, \"unexpected_percent_nonmissing\": 0.4, \"partial_unexpected_list\": [ 16.089981079101477, 12.25, 10.209991455078125, 28.8800048828125, 11.029998779296875, 16.3800048828125, 10.019989013671875, 10.8599853515625 ] }, \"exception_info\": null, \"meta\": {}, \"success\": true, \"expectation_config\": { \"kwargs\": { \"column_expression\": \"abs(open - close)\", \"max_value\": 10, \"mostly\": 0.95 }, \"expectation_type\": \"expect_column_expression_values_to_be_between\", \"meta\": {} } } A column-pair expectation \u00b6 Similarly, you can define a column-pair expectation. Here we check if rounded open equals rounded close : open_close_rounded = playground . expect_column_expression_pair_values_to_be_equal ( column_expression_A = 'round(open)' , column_expression_B = 'round(close)' ) print ( open_close_rounded ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 649, \"unexpected_percent\": 32.45, \"unexpected_percent_nonmissing\": 32.45, \"partial_unexpected_list\": [ [ 78.0, 75.0 ], [ 81.0, 78.0 ], ... ] }, \"exception_info\": null, \"meta\": {}, \"success\": false, \"expectation_config\": { \"kwargs\": { \"column_expression_A\": \"round(open, 0)\", \"column_expression_B\": \"round(close, 0)\" }, \"expectation_type\": \"expect_column_pair_expressions_to_be_equal\", \"meta\": {} } } A multi-column expectation \u00b6 You can also use expressions in multi-column expectations. Here we check that for one symbol and date there should be only one line. For additional fun, we convert the date to a unix timestamp. symbol_date_unique = playground . expect_multicolumn_expression_values_to_be_unique ( column_expression_list = [ 'symbol' , \"unix_timestamp(date, 'yyyy-MM-dd')\" ] ) print ( symbol_date_unique ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 0, \"unexpected_percent\": 0.0, \"unexpected_percent_nonmissing\": 0.0, \"partial_unexpected_list\": [] }, \"exception_info\": null, \"meta\": {}, \"success\": true, \"expectation_config\": { \"kwargs\": { \"column_expression_list\": [ \"symbol\", \"unix_timestamp(date, 'yyyy-MM-dd')\" ] }, \"expectation_type\": \"expect_multicolumn_expression_values_to_be_unique\", \"meta\": {} } }","title":"Expectations With Expressions"},{"location":"settle-dq/developing-expectations/validation/","text":"Validating Your Data \u00b6 After the Data Engineer has added expectations, they need to create validation notebook(s) to validate current data against the defined expectations. The validation notebook should have the following structure. Run the Configuration Notebook % run ./ path / to / conf_notebook Run validation on top of your data: my_table = 'my_db.customers' results = dq_tool . expectation_store . validate_table ( table_name = my_table_name ) results After that, there is a dq_tool variable available to work with the DQ Tool. You'll see the validation result dict in your notebook. The results are written to the Expectation Store database. From there, the Data User can see them using the web application","title":"Validating Your Data"},{"location":"settle-dq/developing-expectations/view-validation-results/","text":"Viewing Validation Results \u00b6 The Data User can see their DQ Agreements in the web app deployed in your infrastructure. They can view the last run of each expectation and see the result details.","title":"Viewing Validation Results"},{"location":"settle-dq/getting-started/","text":"Getting started with Settle DQ \u00b6 The schema above shows the parts of the system as they are used by Settle DQ users. Data Engineer works with a python interface that is packaged as a dq_tool python wheel and installed on a Databricks cluster or notebook. Data User works with a web app in their browser. The common ground is a PostgreSQL Azure database.","title":"Overview"},{"location":"settle-dq/getting-started/azure-setup/","text":"Azure Setup \u00b6 The schema above shows the infrastructure the system operates on. You will need to set up the resources displayed in the Your Azure Cloud rectangle. We recommend creating the resources in the order described in this guide. Prerequisites \u00b6 You will need the following prerequisites to start following this guide. Guest user in DataSentics Azure Cloud \u00b6 You will install Settle DQ code from artifacts that are distributed using DataSentics Azure cloud. DataSentics will invite you as a guest user. For that you will need an email address that can receive emails. We recommend using a technical user, not a real person's email. The access to our Azure will be used for downloading and installing artifacts as a part of the initial setup as well as upgrading to future Settle DQ versions. Follow these steps to create a guest user account: Locate an invitation email in your inbox from invites@microsoft.com Click an Accept Invitation link in the email If this email address doesn't have a Microsoft account yet, it needs to sign up for one. Follow the instructions. You will get another email with a verification link or code. You will be promted to accept permissions for DataSentics to Sign in and Read email address. Click accept. You will get yet another email that welcomes you to Azure Devops Now you will be able to login with this guest account to Subscriber resources in DataSentics Azure cloud. Verify you can log in. Artifact Feed URLs from DataSentics \u00b6 DataSentics will give you the following URLs. You'll use them to download artifacts described in the guide. Python Artifact Feed URL, format https://dev.azure.com/xxx/_packaging?_a=feed&feed=dq_tool%40Local private PyPI URL, format pkgs.dev.azure.com/xxx/_packaging/dq_tool%40Local/pypi/simple/ Universal Packages Artifact Feed URL, format https://dev.azure.com/xxx/_packaging?_a=feed&feed=web_files_zip Web App: Container Registry Credentials from DataSentics \u00b6 For deploying the web app, you will need credentials to Azure Container Registry. From there you will pull a docker image to run in your Azure Web App. You will need: Server URL Username Password Image and tag Azure CLI installed \u00b6 Follow the installation instructions Add the DevOps extension: az extension add --name azure-devops Permissions \u00b6 You will need a subscription in your Azure Cloud where you have permissions to the following Resource providers. The ones that are bold are explicitly needed for the system to be set up and work properly. Microsoft.Databricks Microsoft.DataFactory Microsoft.OperationalInsights Microsoft.KeyVault Microsoft.Storage Microsoft.Network Microsoft.ManagedIdentity Microsoft.Security Microsoft.PolicyInsights Microsoft.GuestConfiguration Microsoft.Compute Microsoft.ContainerService Microsoft.Advisor microsoft.insights Microsoft.MachineLearningServices Microsoft.Purview Microsoft.DBforPostgreSQL Microsoft.Web 1. Resource Group \u00b6 We recommend creating a new resource group for all resources that you will create for Settle DQ. 2. Key Vault \u00b6 You will need an Azure Key Vault to store database connection parameters. If you already have a Key Vault, you can use it. If not create one in your Resource Group. 3. Database \u00b6 The database is the center piece of the DQ Tool. The Python API stores expectation definitions and validations results there. The Web App reads information to display from there. If you already have an Azure PostgreSQL Database server, you can just create a new database there. Connect to the server and create a new database . If you don't have a database server, you'll need to create one. We recommend using Azure Database for PostgreSQL. Follow the quickstart guide . We recommend the following options: Single server PosgreSQL 10 Choose the machine depending on your expected workload, the minimal recommended requirements are 1 vcore, 20GB storage For later steps you will need the following parameters. Store them to your Key Vault . host port database username password 4. Databricks \u00b6 If you're not using Azure Databricks yet, you'll need to create a new Databricks Workspace . If you're already using Azure Databricks and have a workspace, you can just use that. You will need an interactive cluster with Databricks Runtime >= 7.0 to install the wheel to. Your Databricks will need access to Azure Key Vault to retrieve the database credentials. Follow the guide to create an Azure Key Vault-backed secret scope . Call it dbx_scope . Now we'll install dq_tool python wheel to Databricks. The wheel provides a python interface that Data Engineers use to develop and manage data expectations . There are two options how to install the wheel. Option A: Manual Wheel Installation \u00b6 We only recommend using this option when your Databricks workspace doesn't have access to internet. The downside is that you will need to manually download and update the wheel with every new version. Download a wheel from the Pytho Artifact Feed URL you got as a prerequisity Upload the wheel to your Databricks workspace and install the wheel to the given cluster or notebook . Option B: Installation from private PyPI \u00b6 This is an automated way to install the dq_tool package. You don't need to store the wheel anywhere, it will always be installed from DataSentics private PyPI that is a part of an Azure DevOps Artifact Feed. First you need to generate a Personal Access Token (PAT) for your guest user. In the Create a new personal access token dialog you need to provide the following: Organization : settle-dq-ext. If you can't see settle-dq-ext, choose All accessible organizations Scopes: Custom defined In Packaging section check Read Name and Expiration are up to you Save the token and add it to your Key Vault as ds_azure_pat along with the technical user email address ds_azure_email and DataSentics PyPI URL ds_pypi_url you got as a prerequisity. In your notebook, install the library using something like user = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_azure_email\" ) token = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_azure_pat\" ) pypi_url = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_pypi_url\" ) % pip install dq_tool -- extra - index - url = https : // $ user : $ token @ $ pypi_url If you get an error like the one below, it means you don't have permissions for the feed, or your PAT is wrong. WARNING: 401 Error, Credentials not correct for https://pkgs.dev.azure.com/xxx/_packaging/dq_tool%40Local/pypi/simple/dq_tool/ ERROR: Could not find a version that satisfies the requirement dq_tool (from versions: none) ERROR: No matching distribution found for dq_tool Configure private PyPI for a Databricks cluster (Optional) \u00b6 To make installation easier for Databricks users, you can do the following setup. First you have to create a Cluster init script if you don't have one and pass it to the cluster. Add the following code to yor init script: mkdir /root/.pip cat > /root/.pip/pip.conf <<EOF [global] extra-index-url=https://$DS_AZURE_EMAIL:$DS_AZURE_PAT@$DS_PYPI_URL EOF Then configure the cluster environment variables as described below. The variable values will be taken from databricks secrets pointing to KeyVault. DS_AZURE_EMAIL={{secrets/dbx_scope/ds_azure_email}} DS_AZURE_PAT={{secrets/dbx_scope/ds_azure_pat}} DS_PYPI_URL={{secrets/dbx_scope/ds_pypi_url}} After this is done, to install dq_tool to your notebook, you only need to run the basic command below, without having to worry about the extra index url. %pip install dq_tool Verify The Installation \u00b6 Run the following code to verify you can connect to the database. from dq_tool import DQTool dq_tool = DQTool ( spark = spark , db_store_connection = { 'drivername' : 'postgresql' , 'host' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'host' ), 'port' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'port' ), 'database' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'database' ), 'username' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'username' ), 'password' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'password' ) } ) The code creates the database schema if it's not there yet. Then you can start using the python interface as described in the Develop Expectations guide . Save the code, you'll use it in the Configuration Notebook 5. Web Application \u00b6 The Data User checks the validation results in a web application. It needs an Azure Web app and a Storage account. add Web App Create Web App \u00b6 Create a Web App in the Azure portal . Use the following options: Select your Subscription and Resource Group WebApp name : your choice Publish : select Docker Conatiner Operating system : Linux Service Plan : Create new SKU size : select B1 at least Click Next : Docker > Now you'll need to fill in the credentials for a private Docker registry, you got from DataSentics. Options : Single Container Image Source : Private Registry Click Review + create Create Storage Account \u00b6 Create a Storage Account in the Azure portal . Use the following options: Storage account name : Use 5-50 just alfanumeric characters Performance : Standard Account kind : StorageV2 (general purpose v2) Replication : RA-GRS Click Review + create Enable Static Website \u00b6 Now open your newly created Storage Account. On the left side in Settings click Static Website . Use the following options: Static website : click to set Enabled Index document name : index.html Click Save Change Access Level \u00b6 Now Change access level for the $web container, so that it can be served as a website. Open your Storage Account, click Overview . Under Essentials click Containers . A table of containers opens. Click on three dots on the right side of the $web container Click Change access level Choose Blob (anonymous read access for blobs only) Click OK Upload Frontend Files to Storage Account \u00b6 Now you need to download the web frontend static files zip archive. Open the link you got as desribed in Prerequisites. Download the zip directly from the website, or click Connect to feed to download using the CLI. Unzip the archive. Use the az CLI to upload the contets to the Storage account: az storage blob upload-batch -d '$web' -s ./build --account-name <name of storage> Set Web App Environment Variables \u00b6 You need to point the web app to the database and to the Storage account with the frontend files. Open your Web App, on the left side under Settings click Configuration . Add the following env variables: DB_HOST DB_PORT DB_NAME DB_USER DB_PASS FE_CONTAINER_ADDRESS: URL of the Storage, format https://xxx.web.core.windows.net/ Add authentication to the web app \u00b6 The easiest option to enable authentication is the Express option. To enable authentication using the Express option, follow these steps: In the Azure portal , search for and select App Services , and then select your app. From the left navigation, select Authentication / Authorization > On . Select Azure Active Directory > Express . If you want to choose an existing app registration instead: Choose Select Existing AD app , then click Azure AD App . Choose an existing app registration and click OK . Select OK to register the App Service app in Azure Active Directory. A new app registration is created. Click Save Congratulations \u00b6 You can find the URL of the web app in the Web App Overview, usually in format https://xxx.azurewebsites.net Now you can continue with Developing Expectations Guide to start using Settle DQ.","title":"Azure Setup"},{"location":"settle-dq/getting-started/configuration-notebook/","text":"Settle DQ Configuration Notebook \u00b6 The configuration notebook isn't needed when you use Daipe. If you're using Daipe, continue to the Daipe Integration Guide . If not, continue reading. The python interface to Settle DQ is called DQ Tool. It is distributed as a python wheel that you install to your Databricks cluster. To use DQ Tool, you need to configure it so that it knows how to connect to the database. We highly recommend storing this configuration in a single notebook and run the notebook whenever you need to work with DQ Tool. We higly recommend storing connection paremeters in a Databricks secrets service, especially the password. If you already have these in Azure Key Vault, create a Azure Key Vault-backed secret scope in Databricks and read the params from there. If you want to keep the parameters just in Databricks, create a Databricks-based secret scope and store the parameters there. The notebook should contain code like shown in this example: from dq_tool import DQTool dq_tool = DQTool ( spark = spark , db_store_connection = { 'drivername' : 'postgresql' , 'host' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'host' ), 'port' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'port' ), 'database' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'database' ), 'username' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'username' ), 'password' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'password' ) } )","title":"Create Configuration Notebook"},{"location":"settle-dq/getting-started/daipe-integration/","text":"Daipe Integration \u00b6 If you're already using Daipe and want to start using DQ Tool, you need to set it up, as described below. The DQ Tool configuration and instantiation is handled by Daipe, so you can use a DQ Tool instance in any Daipe-decorated notebook function. Setup \u00b6 The following steps assume you already have a daipe project and want to start working with DQ Tool in that project. 1. Add Artifact Feed URL to pyproject.toml \u00b6 The dq-tool-bundle library is distrubuted through a private Datasentics Python package index (artifact feed). Add the following lines to your pyproject.toml so that poetry knows where to get private python wheels. [[tool.poetry.source]] name = \"datasentics\" url = \"<artifact feed url>\" secondary = true The artifact feed url will be given to you along with your token. 2. Set your artifact feed token in poetry \u00b6 Run the following command in your terminal with conda activated: poetry config http-basic.datasentics __token__ <the token> --local Now poetry has credentials to access the artifact feed. 3. Add dq-tool-bundle as a dependency \u00b6 Run the following command in your terminal with conda activated: poetry add dq-tool-bundle Now you have installed the DQ Tool bundle, the DQ Tool and other necessary dependencies. When you deploy to databricks, these will be available there too. 4. Add database connection string to Databricks secrets / Azure Vault \u00b6 Take credentials to your expectation database and store a connection string to your Databricks secrets, or Azure Vault accessible in Datatabricks secrets. The connection string needs to have the following format: postgresql://username:pasword@host:port/database Note that if you have a $ in your password, you'll need to escape it \\$ in the connection string. Next, store the artifact feed token to your Databricks secrets / Azure Vault. Remember the scope and secret names where you have stored these secrets. You'll need them in the next step. 5. Set up cluster environment variables \u00b6 In your databricks cluster you need to set up the following variables. You can find environment variables under cluster configuration -> Advanced Options -> Spark. Set the following variables: DQ_TOOL_DB_STORE_CONNECTION_STRING={{secrets/your_scope/postgres_store_connection_string}} DATABRICKS_HTTP_BASIC_DATASENTICS_PASSWORD={{secrets/your_scope/artifact_feed_token}} DATABRICKS_HTTP_BASIC_DATASENTICS_USERNAME=__token__ You'll need to restart your cluster so that the changes can take effect. Usage \u00b6 Now your can re-deploy your Daipe project and start using DQ Tool. Define expectations \u00b6 To define a new expectation, run a notebook function like the following. from datalakebundle.imports import notebook_function from datalakebundle.table.parameters.TableParametersManager import TableParametersManager from dq_tool import DQTool @notebook_function () def define_expectations_bronze_covid_tbl ( dq_tool : DQTool , table_parameters_manager : TableParametersManager ): # playground lets you run expectation on top of a table params = table_parameters_manager . get_or_parse ( 'my_db.my_table' ) dq_tool . get_playground ( table_name = params . table_identifier , database_name = params . db_identifier ) # the NEVER column values should be between 0 and 1 never_limits = my_playground . expect_column_values_to_be_between ( column = \"NEVER\" , min_value = 0 , max_value = 1 ) print ( never_limits ) For a list of expectations you can define, see the Great Expectations docs After you have fine-tuned your expectation definition, you can save it within a decorated function like this: dq_tool . expectation_store . add ( expectation = never_limits , database_name = my_database_name , table_name = my_table_name , severity = 'error' , agreement = 'The prediction model needs at least 400 rows to predict something meaningful.' , tags = [ 'Data Science' , 'Basic' ] ) Note that this is a throw-away code that doesn't need to be stored in git. All expectation definitions live in your database. To edit expectations, see the editing guide Validate data \u00b6 In your pipeline, you'll want to validate data in a table using expectations you saved in the previous step. Use the following snippet as an example. from datalakebundle.imports import notebook_function @notebook_function () def validate_table ( dq_tool : DQTool , table_parameters_manager : TableParametersManager ): results = dq_tool . expectation_store . validate_table ( database_name = my_database_name , table_name = my_table_name ) print ( results )","title":"Daipe Integration"}]}